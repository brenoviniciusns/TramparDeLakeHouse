{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e926809-d4d3-4e32-b567-25fd2bfd8a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "# Inicializar a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"ReplicateFolderStructure\").getOrCreate()\n",
    "\n",
    "# Definir os caminhos de origem e destino\n",
    "source_path = \"/Volumes/ingestao_dados/tabnews/datalake_tabnews/\"\n",
    "destination_path = \"/Volumes/ingestao_dados/trampar_lakehouse/novodatalake/TabNews\"\n",
    "\n",
    "def list_directories(path):\n",
    "    \"\"\"Lista todos os diretórios em um caminho, incluindo subdiretórios.\"\"\"\n",
    "    dirs = []\n",
    "    for item in dbutils.fs.ls(path):\n",
    "        if item.isDir():\n",
    "            dirs.append(item.path)\n",
    "            dirs.extend(list_directories(item.path))\n",
    "    return dirs\n",
    "\n",
    "def create_directory_structure(source_dir, dest_base):\n",
    "    \"\"\"Cria a estrutura de diretórios no destino.\"\"\"\n",
    "    # Remove o prefixo 'dbfs:' se estiver presente\n",
    "    source_dir = source_dir.replace(\"dbfs:\", \"\")\n",
    "    dest_base = dest_base.replace(\"dbfs:\", \"\")\n",
    "    \n",
    "    # Extrai o caminho relativo\n",
    "    relative_path = source_dir.replace(source_path, \"\").lstrip(\"/\")\n",
    "    \n",
    "    # Constrói o novo caminho de destino\n",
    "    new_dir = f\"{dest_base}/{relative_path}\"\n",
    "    \n",
    "    try:\n",
    "        dbutils.fs.mkdirs(new_dir)\n",
    "        print(f\"Diretório criado: {new_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar diretório {new_dir}: {str(e)}\")\n",
    "\n",
    "# Listar todos os diretórios na origem\n",
    "source_dirs = list_directories(source_path)\n",
    "\n",
    "# Criar a estrutura de diretórios no destino\n",
    "for dir_path in source_dirs:\n",
    "    create_directory_structure(dir_path, destination_path)\n",
    "\n",
    "print(\"Processo de replicação da estrutura de pastas concluído.\")\n",
    "\n",
    "# Verificar a estrutura criada\n",
    "print(\"\\nEstrutura de pastas no destino:\")\n",
    "for item in dbutils.fs.ls(destination_path):\n",
    "    print(item.path)\n",
    "\n",
    "# Mostrar volumes disponíveis\n",
    "print(\"\\nVolumes disponíveis:\")\n",
    "spark.sql(\"SHOW VOLUMES IN ingestao_dados.trampar_lakehouse\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d68b5d22-64f9-46e0-b255-f610d4057f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Inicializar a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"CopyJSONFiles\").getOrCreate()\n",
    "\n",
    "# Definir os caminhos de origem e destino\n",
    "source_path = \"/Volumes/ingestao_dados/tabnews/datalake_tabnews\"\n",
    "destination_path = \"/Volumes/ingestao_dados/trampar_lakehouse/novodatalake\"\n",
    "\n",
    "def list_json_files(path):\n",
    "    \"\"\"Lista todos os arquivos JSON em um caminho, incluindo subdiretórios.\"\"\"\n",
    "    json_files = []\n",
    "    for item in dbutils.fs.ls(path):\n",
    "        if item.isDir():\n",
    "            json_files.extend(list_json_files(item.path))\n",
    "        elif item.name.lower().endswith('.json'):\n",
    "            json_files.append(item.path)\n",
    "    return json_files\n",
    "\n",
    "def copy_file(source, destination):\n",
    "    \"\"\"Copia um arquivo da origem para o destino.\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.cp(source, destination, True)  # True para sobrescrever\n",
    "        print(f\"Arquivo copiado: {source} -> {destination}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao copiar {source} para {destination}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Listar todos os arquivos JSON na origem\n",
    "source_files = list_json_files(source_path)\n",
    "\n",
    "# Copiar os arquivos\n",
    "successful_copies = 0\n",
    "for source_file in source_files:\n",
    "    # Construir o caminho de destino\n",
    "    relative_path = source_file.replace(source_path, \"\").lstrip(\"/\")\n",
    "    destination_file = os.path.join(destination_path, relative_path)\n",
    "    \n",
    "    if copy_file(source_file, destination_file):\n",
    "        successful_copies += 1\n",
    "\n",
    "print(f\"Processo de cópia concluído. {successful_copies} arquivos copiados com sucesso.\")\n",
    "\n",
    "# Verificar o número de arquivos JSON no destino\n",
    "def count_json_files(path):\n",
    "    \"\"\"Conta o número de arquivos JSON em um caminho, incluindo subdiretórios.\"\"\"\n",
    "    count = 0\n",
    "    for item in dbutils.fs.ls(path):\n",
    "        if item.isDir():\n",
    "            count += count_json_files(item.path)\n",
    "        elif item.name.lower().endswith('.json'):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "num_files_destination = count_json_files(destination_path)\n",
    "print(f\"Número de arquivos JSON no destino: {num_files_destination}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Copiar Arquivos de um DataLake para outro Datalake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
